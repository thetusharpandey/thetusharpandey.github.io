Morphological Classification of Galaxies using Conv-Nets
 Mamta Mittal1, Maanak Arora1, Tushar Pandey1, *
1 Department of Computer Science and Engineering, G. B. Pant Government Engineering College, Okhla, New Delhi
mittalmamta79@gmail.com, maanakarora@gmail.com, thetusharpandey@icloud.com 
* Corresponding Author: thetusharpandey@icloud.com

Abstract. Since the beginning of space exploration, the galaxy classification has been a vexed problem that has always muddle the astrophysicists. A number of techniques have proven their remarkable utility in classification of galaxies, however, upon analysis these methods revealed certain inefficiencies which cannot be overlooked. Traditional classification of galaxies in the universe contain a significant part of their history in the authority of government agencies where the classifications in the previous years was performed primarily by experts manually. Today's astronomical research produces large amounts of data and manually labelling the galaxy images based on morphological features can be time-consuming and error prone. The objective has been to study and analyse the different types of machine learning algorithms/methodologies used for classifying galaxies. An inference drawn from this study is that using deep learning algorithms in conjunction with some Data Augmentation techniques provide excellent classification results of galaxies. Considering the aforementioned fact, the authors have proposed a classification model for classification of galaxies morphologically using the deep convolutional neural networks along with certain data augmentation techniques. “The Galaxy Zoo” dataset has been used from Kaggle which is further handcrafted for ease of classification. The custom dataset derived from Galaxy Zoo dataset has 11,494 images which is further split into Training and Validation set in a 70:30 ratio. The galaxies are classified into three classes only: spiral, elliptical and somewhere in-between (lenticular). The proposed architecture brings Convolutional Neural Networks into use due to their record of good results with classification tasks among images. The efficient implementation of this method is capable of providing a testing accuracy of up-to 88.4%, which make the proposed model outperform its earlier contemporaries.


Keywords: Galaxy classification, Galaxy morphology, Deep Convolutional Neural networks, Elliptical Galaxy, Spiral Galaxy, Lenticular Galaxy.



Introduction 
	
    A galaxy coasting freely in the interstellar space in modern day is a substance of enthusiasm for pretty much every astronomer and researcher who wish to retaliate for the limits of human comprehension of the said degree of the observable universe. These structures, called galaxies unlike viewed as from the earth, scale to tremendous measurements going up to a few hundreds and even thousands of light years. A galaxy is a cluster, a system of huge counts of stars, planets, asteroids and a seemingly humungous quantity of gases, space dust and other forms of matter. Studying about the structure and characteristics of galaxies proves its advantages in the understanding of the universe and its origins. Galaxy morphological classification is the process of clustering the galaxies into different groups by the virtue of their physical characteristics The division of galaxies into groups based on their visual appearance is called galaxy morphological classification. Several methodologies are in use today by which galaxies can be categorised into different classes based on their morphologies. 
    As a consequence of this search for questions regarding heavenly bodies, astronomy is a research field ripe with large datasets [1]. Simultaneous to the enormous datasets, machine learning methods are rapidly becoming a go-to tool for automating data-intensive processes which normally require days or weeks of often tedious human processing [2]. Due to the incomprehensible size of the universe, classifying celestial bodies is an excellent candidate for the application of machine learning. 
    Classically, morphological classification of galaxies is a case of examining 2D pictures of the galaxies and categorising in respective order. Although human expertise in classification tasks is somewhat reliable, it is extremely time consuming to classify large quantities of astronomy related databases recently gathered due to the increased sophistication and dimensions of telescopes and the Charge-Coupled Device cameras (CCD). This led on to the production of huge galaxy related datasets ,for example, the Sloan Digital Sky Survey (SDSS) [3]. This data is manually infeasible to analyse due to the humongous size of the dataset. Thus classification has been a goal for a long time for the astrophysicists. No matter how much effort is put into successful classification of the images, complicated nature of the scans and the low resolution of the galaxies prove that the task is very challenging and somewhat inaccurate.
    Classification systems help the astronomers to classify and group the portion of the sky into regions of interest and allow the organisations to better channel the effort of study or analysis in to a highly tailored domain and not into entire region of observation.
    This paper demonstrates how a CNN can be constructed to analyze images of galaxies to automatically detect metrics that reproduce the probability distributions derived from expert and crowd-sourced human classifications. The dataset comes from a number of resources e.g. Sloan Digital Sky Survey [4-6], Galaxy Zoo dataset on Kaggle [7-9] and various resources in the public domain. The datasets and sources referred have have previously been used in a number of similar studies, generating results in a varied range of accuracy as seen in table 1. 


Table 1. A comparison of various Machine Learning algorithms used in morphological classification of galaxies.
 
#####The following section contains various methods used in galaxy classification. Section 2 extends the reach of previous section as it discusses the techniques and approach used in the paper to realise the model. Section 3 discusses the dataset used and the techniques for preparing the data into the form it was passed on to the model. Section 4 delineates the presented sequential model utilized here to classify galaxies. Section 5 states the results obtained with graphical analysis and visualisation of the model performance.  The paper has been concluding in the succeeding section followed by a list of references of the material and theory that proved to be really helpful in the development of this model.

2. Proposed Method

This method uses four convolutional layers and all of these layers are accompanied by a max-pooling layer. // The dataset used along with pivotal data acquisition steps and data pre-processing steps have been defined below. Subsequently, various deep learning methodologies used by the model and complete model architecture have been elaborated. 

2.1 Dataset

   The authors have used the “Galaxy Zoo” dataset which contains a total of 141,553 images of different galaxies. All these images are split into 61,578 images for the purpose of training, with each section having a unique probability distribution respective to that particular division of inputs and 79,975 other images for testing purpose. The dataset’s images are classified into different categories of galaxies. With every category having attributes that are ranked by the  volunteers, the original dataset contains 37 separate categories. While these numbers correspond to the characteristics of the dataset in original form, the proposed model uses an excerpt from the dataset thereby using 8,040 images for          the training dataset. Among the images available in the dataset so generated, some categories can be interdependent for instance the quantity of spiral arms and extent of spiral tightness depends upon whether the galaxy is a spiral or not?    [10 -12]
  In the later stages the votes for these categorisations provided by the volunteers are normalized to a number between 0 and 1 [13]. A higher number indicates higher number of volunteers marking that vote with higher level of confidence, while numbers closer to 0 display the opposite case of lesser volunteers voting with a lower confidence for that attribute. These numbers represent the morphology of a galaxy from any one of the 37 existing categories In the fig 1, these values have been calculated just for the chosen examples from the evaluation set in real-time, 50 per cent of test participants answering the questions. Also, numerous examples that are present in this subset are listed along-with. The answered questions contribute to a value for the precision and recall and are helpful in determining the class of the galaxy in the original Galaxy Zoo dataset [14].  In this paper the authors have developed a model that morphologically classifies galaxies in three major categories namely Elliptical, Spiral and Lenticular Galaxies(somewhere in between elliptical and spiral) as shown in fig 1.
Figure 1: Examples of galaxies corresponding to each class.



In order to train the classification model a set of images is required for each of the three classes. In this case from the dataset, elliptical galaxies are chosen, but only the images for which 80% of the volunteers were confident in them being a smooth galaxy instead of having other abstract features or a disc like formation in the structure, additionally at least 40% of the volunteers considered the galaxy to be completely round. Similar approaches have been employed to group images from other classes as well. Fig 2 illustrates some examples of the galaxies hence grouped.
























































Figure 2: Precision and recall scores of answers

It should be specified in advance that galaxies are identified in the result stage at the dense layer by an encoding scheme in the model and hence the classifications do to follow a straight forward image to type correspondence format of training, rather a systematic paradigm is laid out and this gives a structural output at the last layer of 3 neurons of the model. These outputs can be one of  variations, but only three such unique combinations have been used to represent one of the three galaxy types. The galaxies have been allotted corresponding outputs as follows:
[1 0 0] T  corresponds to all Elliptical Galaxies
[0 1 0] T  corresponds to all Irregular Galaxies 
[0 0 1] T corresponds to all Spiral Galaxies.

2.2 Data augmentation

   While there is a sufficient count of images to train on, it is possible to improve the performance of the network by augmenting training data, increasing the apparent size of the dataset to the CNN, thereby reducing the chances of overfitting. Since the galaxies are rotationally invariant in the observations hence this dataset is a suitable candidate for data augmentation [15,16]. 
  When the training was completed using the existent data, the validation accuracy was observed to take a sharp dip after a certain point, while training accuracy remained to be constantly rising. It seems to indicate that the model develops a tendency to develop a bit of an over-fitting,  approaching 100% in the training accuracy, while the accuracy in the validation set stagnating around 85%. To try to overcome this problem the authors have used a technique called data augmentation, i.e. generating batches of images by randomly applying some image transformation increasing the variance of the training set for each training epoch. 
   Scale jittering is a popular type of data augmentation. Before training the model, the images were cropped to a range scale S = [150,150], widely called multi-scale training images because of the S random value. Both different and same images can be cropped to distinct sizes in different iterations, making it beneficial to take this into account during the training phase [17]. 

2.3 Data preprocessing

   Dimensions of every image contained in the dataset are  and contain the object of interest in the centre always. For a reduction in the dimensionality of the image, at the time of preprocessing the images are reduced to a dimension of , which is 50% of their original size. Furthermore, down sampling of the images to half the resultant size is performed again into , hence eliminating all unnecessary information present in every image which may pollute the information or interpretations of the network in the later stages. Down sampling may prove to be beneficial to the CNN for learning about those regions who are related to every particular expression as well as help in increasing the performance during training. Down sampling also helps in decreasing the storage size of the information and also assists in the stages of training and learning of the model [18,19]. This preprocessing scheme can be seen into effect in fig 3.




 








Figure 3. Preprocessing schema of the galaxies

2.4  Deep learning

    To detect numerous levels of distributed impersonations is the primary goal of deep learning algorithms. It is a subset of machine learning in Artificial Intelligence (AI). Algorithms used in deep learning remain kindled by the structure and mimic the functions of the human brain, hence referred to as Artificial Neural Networks. The term deep learning is predominantly concerned with large deep neural networks [20-22]. Typically, “deep” translates to the number of layers and hence deep neural networks is the general notion behind the term deep learning. Neural nets are highly scalable which indicates that results improve with abundant data and bigger models, that consequently requires more computation to train. Besides scalability, another extremely useful advantage of deep learning is its potential to automatically extract features from raw data. This is known as “feature learning.” Lately, a number of involving various deep learning algorithms applied in order to provide a solution to some standard artificial intelligence related problem statements.  Basically, a primary goal of learning is to extract abstractions of high levels from the available primary unstructured or untreated data. Such approach towards knowledge development is based on architectures having a hierarchy in their design, interestingly such an approach has been used in the field of AI since a long period of time, such as transfer learning, semantic parsing, computer vision and natural language processing (NLP). The hierarchical concepts allow the computer to learn complex concepts by constructing them out of simpler ones. A graph drawn to show how these concepts are constructed on top of each other, turns out to be deep, with numerous layers. This is why this approach to AI is called deep learning [23]. A feed-forward neural network or Multilayer Perceptron (MLP) is a quintessential example of a deep learning model, Which brings up the topic of Neural Networks.
Neural Network

   A standard Neural Network (NN) consists of many simple, connected processing points called neurons that are used to make predictions, classify things etc. Real-valued activations are produced by each neuron [24]. A neuron can be activated from an input or another neuron’s activation through its weighted connections from a previous layer. An activation function computes a weighted sum of its input, adds a bias and decides whether the neuron’s value should be propagated or not. Rectified Linear Unit (ReLU) is one of the most widely used activation function [25]. A ReLU is defined as: (x) = max(0, x), where x is the weighted sum of the inputs. It returns an output x if (x) is positive, 0 otherwise. NN’s using primarily ReLU activation functions have been shown to enable faster training even with many layers A loss function defines the difference between the target and actual output [26]. A Neural Network uses the loss function to correct weights after a feed forward operation, this process of correction is called back propagation. Next, the error is propagated backwards from the output layer propagating through the hidden layers to the input layer, wherein the weights and biases have been modified in such a way that the error for the most recent input is minimised. Over many training samples the loss function will minimize error and the value of the weights for the whole network will begin to converge [27]. The curve which the error rate of the network experiences is controlled through a gradient descent method. This method can help determine whether the network should be trained further or to stop early. A whole net of interconnected neurons makes a Neural network, which depending on different factors can take up different forms. Due to the structure providing convolutions easy to realise and thereby effectively understand image related datasets and compute on it, the authors in the presented model have employed a CNN model.

Convolutional Neural Network (CNN)

   A CNN is a feed-forward, deep neural network which has the ability to extract elementary visual features from its input. Hubel and Wiesel’s discovery led on to the creation of CNNs, where they were able to find that a cat’s visual cortex has locally sensitive, orientation-selective neurons. CNNs were first introduced in 1999, and since then have been applied to solve numerous different variations of problems in natural language processing, image recognition, and recommendation systems. CNN is a deep learning algorithm which has the capability to take images as inputs and assign importance(i.e. learnable weights and biases) to distinct aspects of the images thereby differentiating one from the other. The most important role of a conv-net is to reduce the images in an easily processable form without losing key features responsible for getting an accurate prediction [28]. 
   The term “convolution” in terms of CNNs, is a linear operation that includes the multiplication of input data and a two-dimensional list of weights, commonly known as a filter or a kernel. Applying this filter to an image will result in a feature map that only contains desired extracted features. The notion of using the convolution operation in a neural network is that the values of the kernel are weights to be learned during the training of the model (neural network). This is how the network learns what kind of features are to be extracted from the input.

Dropout

   A technique called dropout is used for regularization of the data in the layers of the model. The weights that associate the neurons of the previous layer to the succeeding layer can be valued explicitly to zero using this feature. After this cycle to random value-modifications the respective values of the neurons are multiplied with the values of the input to preserve and transfer forward the values into the model for processing. Due to dropouts the node ultimately achieves a more robust and enhanced stance against relying on other nodes in the previous layer for the performance and hence makes the phenomenon of overfitting less probable than before. As a result, in the presented model, the neurons can pick up seemingly unknown data every time and not be forced into a temptation of just learning exact data, which is called overfitting. Since overfitting is so undesirable, a comment on the nature of overfitting is required, including the behaviour it attains in a program development process. An overfitting model is rather a confused or overwhelmed model which is overloaded with data lot more than it can handle or effectively understand, as a result the model develops a tendency to learn the exact data in the training, resulting in a high score in the training accuracy but losing the performance in the validation accuracy [29].

2.5 Model Architecture

  An architecture for the model to be used for morphological classification of galaxies has been depicted in detail in      fig 4. It defines the layers of the proposed model, with the graphical representation of the layers, as well as their respective dimensions. The model has ten Sequential layers, with first layering the main element for feature extraction, followed by various pooling and convoluting layers.
  All viewpoints provided to the network as three arrays each contain one of the RGB color’s information, with the scale of values scaled to between 0 and 1 [30]. The feature maps so developed were simply concatenated and passed off to a stack of fully connected convolution layers which map them to the final dense layer for the interpretation of the answer as is illustrated in the  fig 4. The model is sequential in structure with only convolutions being generated at each step, hence the name, “Convolutional Neural Network”. The given CNN model as discussed already, is a 10 layer Sequential model taking an input of a matrix of depth 3 for all the three values of RGB pixels each and hence to incorporate the visual data about the galaxies in all the 3 standard channels. The data then proceeds through a max pooling layer of , thereby converting the input parameters into a stream of . This stream of data so created out of the resultant information then proceeds to the second layer of convolution where the system obtains additional data parameters from the computing, max pooled by another layer into a further additional convolution so as to obtain 1,47,584 parameters at this point. 
 The model is later flattened and applied a dropout to address the issue of overfitting .In the model, ReLU has been incorporated to minimize the number of links formed to the next convolution layer (and hence the no. of parameters as well). The authors didn’t utilize “max-out” in the further layers since it is too intensive on the computational requirements. 

Figure 4. The proposed sequential model with all the layers connected in order.

 The proposed design was selected after a manual search to ensure maximum efficiency such that the model score is of the highest value. The network has about 3.4 million parameters that can be fully trained. The 37 values ​​generated by the input image network get converted into a set of probabilities. First, the values ​​from the conversion into probabilities  were passed into non-linear rectification, and then the normalized values for each question were later obtained to determine the probability distribution categorically for each question as is discussed later in section 5.3 ####. A viable value of probability distribution could also be achieved by using a soft-max function for each question, instead of a rectification plus normalization approach. But this is seen to inflict a reduction in overall performance because it is too difficult for the network to make a prediction or exactly a value of 0 or 1. However, a rescaling of the distribution is still needed; they provide the probability of an answer conditionally dependent upon the corresponding question, but each user replies to a specific limited set of questions. This also implies that some questions were only presented to a specific set of people, so the probability score of such questions should be well scaled in order to obtain a set of probabilities totally unconditional, independent of such constraints [31].

Activation Function 

   In most of the neural networks activation functions can be seen in action. These activation functions attached to the node of any model define the output of that node depending on a given set of inputs and the resulting outcome from the node attached . Performing like a standard digital circuitry, these functions can be seen as a framework responding in ON or OFF forms dependent on the input provided. But only non linear type of activation functions however allow  such networks to compute the outputs for neural networks effectively using a small number of nodes. Various activation functions are taken in regular use in such networks, depending upon the requirement of specific Node or layer as can be seen in  fig 5. The functions used in the model specifically are Soft-max and ReLU.















                        (a) SoftMax Activation Function  				                  (b) ReLU activation Function 
Figure 5. Activation Functions used in the proposed model.
	


3. Training and Implementation
  
   To train the model, authors have used a batch size of 32. According to the Test-Train dataset split prior to the commencement of development of model, in the preprocessing and dataset phase, the model has been provided with approximately 8040 images. The sequential model consists of one input layer of convolutions, three hidden layers all of which are convolutional in nature with a max pooling operation provided to them in the form of individual max-pool layers, two dense layers and one layer to receive the output. A  filter has been taken, with the batch size of 32 and an input size of  for the images to the model. The Sequential model has been trained on 15 epochs with indicative markers of details of testing accuracy, validation accuracy, time taken in seconds per epoch and testing and validation loss. 
The fig 4 elucidates a graphical illustration of the structure of the entire model, built in layers of CNN blocks and pooling layers of various suitable dimensions. Initially, an input picture of  pixels of three individual colors  has been given in form of input to the first CNN layer. The kernel size of this layer is , which provides about 1,792 parameters to the network. Output from this layer, the 2nd convolutional layer additionally provides after output exactly 32 feature maps with exactly 36928 parameters. Within this arrangement, the 1st max-pooling layer has already been utilized. Similarly, the 3rd conv layer (i.e. conv2d_2) gives  sized output each while the third layer generates a  output after max pooling. All three convolutional layers have used the ReLU function as “activation Function” for inculcating a non-linear behaviour. This same arrangement of convolutional layer with max-pooling is again used for the 4th layer giving an output of . Passing on the data sequentially as the model structure specifies already, the data stream now reached another max pooling layer, giving off an output of  resulting in 6272 parameters. The data is subjected to a flattening followed by a dropout of magnitude 0.5, which is done to prevent the tendency of overfitting in the model if any. Until this point, all the layers use ReLU for the activation function. Dropout of 0.5 means 50% of random neurons are deliberately set to zero. The resulting set of neurons are few to a dense layer, giving out 512 unique neurons in a layer form. It is to be noted that the activation function used in this dense layer is still ReLU. This set of resulting 512 neurons are passed forward to the ending layer where they are supplied as input to the second fully connected dense layer. This time however, instead of the ReLU algorithm, the authors leverage the softmax algorithm to get the probability for each galaxy. The ending node is a soft max module that outputs a probability matrix and the neuron with the greatest probability is finally fired thereby classifying the input galaxy image to the highest probable class. The weights from the model are saved into a file of “.h5” format to be later used for making predictions on the test data set. These weights can be accessed without the need of training the model again, and can be transported as a piece of code to other programs where classification of galaxies is required. 
A detailed and structured representation of the construction of the model can be seen in fig 6 below.





















Figure 6: Model summary of the proposed model

  All the aspects of the model are trained using python as the language of choice upon Keras, which exists as an external but very effective wrapper to the Tensorflow library developed by Microsoft. Due to physical limitations of available hardware, the model is trained on the cloud, using a cloud processing and research platform owned by Google. Inc, namely Google Colab. Colab is a Google research project created to help disseminate machine learning education and research. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.
   A GPU was allocated initially for the training, however it was observed that the training could be completed fasted on a CPU instead. As a result, CPU training was chosen over GPU. A possible reason for this unusual phenomena is suspected to be highly problem specific, as the past evidences prove, some problems are so formulated that they are faster to be processed by CPUs than GPUs. 
   It should be kept in mind that CPU are designed to effectively compute over what is called compute primitives of  data, however GPUs are optimised in structure to process vector units i.e  data units at once, much well   followed by TPUs which are an oncoming trend due to extremely high capabilities, limited only to high cost of operating, maintenance and ownership but can compute over Tensors of  format. The training took around 600 seconds for completion of one epoch, and model was trained for a total of 15 epochs. Therefore, the model took approximately 150 minutes (i.e. 2 hours and 30 minutes) for training. 

4. Results
	   
   This section summaries the results of training of the model on the custom dataset, over a successful training cycle of 15 epochs. The training and validation accuracy of the model is shown in  fig 7 in the form of a graph.
















Figure 7. Plot of Training accuracy and Validation accuracy

   The training accuracy is approximately 90.2% and the validation accuracy turned out to be 88.3% as the average of all epochs run on the model. Fig 8 illustrates the training loss and validation loss.

















Figure 8. Plot of Training Loss and Validation Loss


   Corresponding to accuracy parameters, there exist the evaluation parameters called training loss and validation loss. These are used to monitor the loss taken by the network at the time of training the model and they are actively utilised as indicators of whether the model is overfitting or not. Further, the predictions generated have been illustrated in the present section. The test images can be seen in fig 9 and their corresponding predictions in fig 11 respectively.





















Figure 9. Random test images

   The final dense layer in the network generates a  output. The activation function “soft-max” used in the final dense layer generates a probability distribution matrix of  dimension as the output. The highest probable class in the probability matrix is the class to which the test image belongs. Accordingly we have the following distribution for the classifications as shown in fig 9. The outputs are mapped to one of the three pre-specified classes of galaxies namely,  Spiral, Elliptical and Lenticular as shown in fig 10.
















Figure 10. Mapping of model's output to galaxy classification





           (a)Model prediction:[1,0,0], indicating elliptical galaxy.               (b)Model prediction:[0,0,1], indicating a spiral galaxy.

figure 11. Output generated by the model for random test images



As can be seen from the results above, the galaxies are given as input to the model, and the model generates output in the form of a matrix. The output for  fig 11.(a) is [1,0,0], which corresponds to elliptical galaxy. Similarly for  fig 11.(b), the output is [0,0,1], which indicates a spiral galaxy. 























5. Conclusions 

   The proposed model uses network architecture based on Convolutional Neural Networks and augmentation of data, to classify a set of galaxies into three primary classes namely, Elliptical, Spiral, and Lenticular. The proposed model has been found to achieve an average training accuracy of 90.2% and  an average validation accuracy of 88.3% over a span of 15 epochs. In data preprocessing, an entire preprocessing format has been used along with data augmentation to avoid overfitting. In order to avoid any chance of overfitting, dropouts of magnitude of 0.5 has been used in the network. The model has 10 layers operating from an input of an image of  dimension in three layers of RGB color scheme. The model consumes all the input data to finally operate on 147,584 trainable parameters and runs for 15 epochs keeping the input batch size as 32. The training accuracy and validation accuracy after close monitoring have been found to successfully avoid overfitting at any point in the model’s training cycles and the testing accuracy of the model was reported to be 88.3%. To understand what the CNN model learns, the authors analyse the functioning of each layer in the proposed model. The first layer filters, for instance, identify the various galaxy edges, corners, etc., directly from the original pixel, then edges were used to identify simple shapes in  the filters from second layer, and finally these shapes detect further advanced features in filter layers of higher levels. It has also been witnessed that after pooling layers, each feature map possessed a stronger differentiability, which exactly coincided with the expectations of the proposed classification model. Future attempts of improving the model can be extended to focus on finer-grained morphological classifications of galaxies by using transfer learning, more sophisticated deep learning algorithms or by using larger and higher quality astronomical datasets.  


































6. References
	
[1]  M. Abd Elfattah, N. El-Bendary, M. A. Abu Elsoud, A. E. Hassanien, and M. F. Tolba, An intelligent approach for galaxies images classification, in 13th International Conference on Hybrid Intelligent Systems (HIS 2013), 2013, pp. 167172.  
[2]  M. Abd Elfattah, N. Elbendary, H. K. Elminir, M. A. Abu El-Soud, and A. E. Hassanie n, Galaxies image classification using empirical mode decomposition and machine learning techniques, in 2014 International Conference on Engineering and Technology (ICET), 2014, pp. 15. 

[3]  Kyle W. Willett, Chris J. Lintott, Steven P. Bamford, Karen L. Masters, Brooke D. Simmons, Kevin R. V. Casteels, Edward M. Edmondson, Lucy F. Fortson, Sugata Kaviraj, William C. Keel, Thomas Melvin, Robert C. Nichol, M. Jordan Raddick, Kevin Schawinski, Robert J. Simpson, Ramin A. Skibba, Arfon M. Smith, and Daniel Thomas. Galaxy zoo : detailed morphological classifications for 304 122 galaxies from the sloan digital sky survey. Monthly Notices of the Royal Astronomical Society, 435(4):2835–2860, 2013. doi: 10.1093/mnras/stt1458. URL http://dx.doi.org/10.1093/ mnras/stt1458. 

[4]  N. M. Ball, R. J. Brunner, A. D. Myers, and D. Tcheng. Robust Machine Learning Applied to Astronomical Data Sets. I. Star-Galaxy Classification of the Sloan Digital Sky Survey DR3 Using Decision Trees. Astrophysical Journal, 650:497–509, October 2006. doi: 10.1086/507440.   
[5]  S. Srinivas, R. K. Sarvadevabhatla, K. R. Mopuri, N. Prabhu, S. S. S. Kruthiventi, and R. V. Babu, A Taxonomy of Deep Convolutional Neural Nets for Computer Vision, Front. Robot. AI, vol. 2, p. 36, 2016.  
[6] H. Jiang and E. Learned-Miller, Face Detection with the Faster R-CNN, in 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), 2017, pp. 650657.  

[7] J. De La Calleja and O. Fuentes, Machine learning and image analysis for morphological galaxy classification, Mon. Not. R. Astron. Soc., vol. 349, no. 1, pp. 8793, 2004. 

[8] Zooniverse. The science behind the site. https://www.zooniverse.org/projects/zookeeper/ galaxy-zoo/about/research. 

[9] Baillard, A., Bertin, E., de Lapparent, et. al.: Astron Astrophys 532 (2011) 

[10] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proc. IEEE, vol. 86, no. 11, pp. 22782324, 1998.

[11]  A. Adams and A. Woolley, Hubble classification of galaxies using neural networks, Vistas Astron., vol. 38, pp. 273280, Jan. 1994.  
[12]  A. Dominguez, A History of the Convolution Operation [Retrospectroscope], IEEE Pulse, vol. 6, no. 1, pp. 3849, Jan. 2015.  
[13]  Sifre L., Mallat S., 2013, in Gerard M., Ramin Z., eds, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination. IEEE, New York, p. 1233  
[14]  Robert J Brunner Edward J. Kim. Star-galaxy classification using deep convolutional neural net- works. Monthly Notices of the Royal Astronomical Society, 464:4463–4475, 02 2017.  
[15]  Andries P. Engelbrecht. Computational Intelligence: An Introduction. Wiley Publishing, 2nd edition, 2007. ISBN 0470035617.  
[16]  Stanislav Fort. Towards understanding feedback from supermassive black holes using convolutional neural networks. Workshop on Deep Learning for Physical Sciences (DLPS 2017), 2017. URL https://arxiv.org/pdf/1712.00523.pdf.  
[17]  Mehdi Ghayoumi. A quick review of deep learning in facial expression. Journal of Communication and Computer, 14:34–38, 2017.  
[18]  Tony Hey, Stewart Tansley, and Kristin Tolle. The Fourth Paradigm: Data-Intensive Scientific Discovery. Microsoft Research, October 2009. ISBN 978-0-9825442-0-4.  
[19]  Andrew Zisserman Karen Simonyan. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.

[20]  Alex Krizhevsky, Ilya Sutskever, and Georey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors,   25, pages 1097–1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf. 

[21]  Yann Lecun, Patrick Haner, L ́eon Bottou, and Yoshua Bengio. Object recognition with gradient based learning. In Contour and Grouping in Computer Vision. Springer, 1999. 

[22]  Vinod Nair and Georey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, pages 807–814, USA, 2010. Omnipress. ISBN 978-1-60558-907-7. URL http://dl.acm. org/citation.cfm?id=3104322.3104425.  
[23]  Alex Krizhevsky Ilya Sutskever Ruslan Salakhutdinov Nitish Srivastava, Georey Hinton. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958, 2014.  
[24]  Beth Willman Ross Fadely, David W. Hogg. Star–galaxy classification in multi-band optical imaging. The Astrophysical Journal, 760(1):15, November 2012

[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.  
[26]  Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. Deep content-based music recommendation. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2643–2651. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/ 5004-deep-content-based-music-recommendation.pdf. 

[27]  Y. Wu, Y. Fang, X. Ren, and H. Lu, Back propagation neural networks based hysteresis modeling and compensation for a piezoelectric scanner, in 2016 IEEE International Conference on Manipulation, Manufacturing and Measurement on the Nanoscale (3M-NANO), 2016, pp. 119–124.  
[28] R. Fontanella, D. Accardo, E. Caricati, S. Cimmino, and D. De Simone, An extensive analysis for the use of back propagation neural networks to perform the calibration of MEMS gyro bias thermal drift, in 2016 IEEE/ION Position, Location and Navigation Symposium (PLANS), 2016, pp. 672–680. 

[29]  A. Baillard et al., The EFIGI catalogue of 4458 nearby galaxies with detailed morphology, Astronomy & Astrophysics, Volume 532, id.A74, 27 pp., vol. 532, Mar. 2011.  
[30] M. Marin, L. E. Sucar, J. A. Gonzalez, and R. Diaz, A Hierarchical Model for Morphological Galaxy Classification, in Proceedings of the Twenty-Sixth International Florida Artificial Intelligence Research Society Conference, 2013, pp. 438–443 

[31] H. Habibi Aghdam and E. Jahani Heravi, Guide to Convolutional Neural Networks. Cham: Springer International Publishing, 2017.  
[32] I. Sa, Z. Ge, F. Dayoub, B. Upcroft, T. Perez, and C. McCool, DeepFruits: A Fruit Detection System Using Deep Neural Networks, Sensors, vol. 16, no. 8, p. 1222, Aug. 2016.  
